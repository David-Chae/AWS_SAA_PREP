
# â­ Amazon Kinesis Data Firehose ì‰½ê²Œ ì´í•´í•˜ê¸°

FirehoseëŠ” **AWSê°€ ì œê³µí•˜ëŠ” â€˜ìë™ ë°ì´í„° íŒŒì´í”„ë¼ì¸â€™**ì´ë¼ê³  ìƒê°í•˜ë©´ ë¼.
**ì‹¤ì‹œê°„ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ë°ì´í„° â†’ í•„ìš”í•˜ë©´ ê°€ê³µ â†’ S3 ê°™ì€ ì €ì¥ì†Œë¡œ ìë™ ì €ì¥**
ì´ ê³¼ì •ì„ â€˜ì•Œì•„ì„œâ€™ í•´ì£¼ëŠ” ì„œë¹„ìŠ¤ì•¼.

---

# 1) FirehoseëŠ” ì–´ë””ì„œ ë°ì´í„°ë¥¼ ë°›ë‚˜? (ì…ë ¥)

ë°ì´í„°ë¥¼ ë³´ë‚´ëŠ” ìª½ì„ **Producer(ìƒì‚°ì)**ë¼ê³  í•´.

FirehoseëŠ” ì•„ë˜ ê°™ì€ ì—¬ëŸ¬ ê³³ì—ì„œ ë°ì´í„°ë¥¼ ë°›ì•„ì˜¬ ìˆ˜ ìˆì–´:

* **Kinesis Data Stream** (ê°€ì¥ í”í•¨)
* **CloudWatch Logs**, **EventBridge**, **IoT Core** ê°™ì€ AWS ì„œë¹„ìŠ¤ë“¤
* **Direct PUT** (ì•±ì—ì„œ SDKë¡œ ì§ì ‘ Firehoseì— ë³´ë‚´ê¸°)
* **Kinesis Agent** (ì„œë²„ ë¡œê·¸ ë“±ì„ ìë™ìœ¼ë¡œ ì „ì†¡)

ğŸ‘‰ í•µì‹¬: **ë‹¤ì–‘í•œ ê³³ì—ì„œ ë“¤ì–´ì˜¤ëŠ” ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ Firehoseê°€ ìë™ìœ¼ë¡œ ë°›ì•„ì¤Œ**

---

# 2) í•„ìš”í•˜ë©´ ê°€ê³µ(Transform) ê°€ëŠ¥ â€“ Lambda

ë„ì°©í•œ ë°ì´í„°ë¥¼ **Lambdaë¡œ ë³€í™˜**í•  ìˆ˜ ìˆì–´.

ì˜ˆ:

* í˜•ì‹ì„ JSON â†’ CSVë¡œ ë°”ê¾¸ê¸°
* í•„ìš” ì—†ëŠ” í•­ëª© ì œê±°í•˜ê¸°
* ì••ì¶• í’€ê¸° / ë‹¤ì‹œ ì••ì¶•í•˜ê¸°
* í…ìŠ¤íŠ¸ ì •ë¦¬í•˜ê¸° ë“±

ğŸ‘‰ ë³€í™˜ì€ **ì„ íƒì‚¬í•­**ì´ì§€ë§Œ, ë°ì´í„° ì •ë¦¬í•  ë•Œ ìœ ìš©í•¨.

---

# 3) Firehoseê°€ ë°ì´í„°ë¥¼ ë³´ë‚´ëŠ” ê³³ (ì¶œë ¥)

FirehoseëŠ” ë°ì´í„°ë¥¼ ë‹¤ìŒ ê°™ì€ ì €ì¥ì†Œë¡œ ìë™ ì ì¬í•´:

* **S3** (ê°€ì¥ ê¸°ë³¸ì )
* **OpenSearch Service**
* **Redshift**
* **HTTP ì—”ë“œí¬ì¸íŠ¸**
* ì¼ë¶€ 3rd party ì„œë¹„ìŠ¤ë“¤

ğŸ‘‰ ì‹¤ë¬´ì—ì„œ ì¤‘ìš”í•˜ê²Œ ì•Œì•„ì•¼ í•  ê±´ **S3 / Redshift / OpenSearch** ì´ 3ê°œ.

---

# 4) Delivery Stream ë§Œë“¤ê¸° (ì‹¤ìŠµ íë¦„ ì´í•´)

ë”± ì´ ìˆœì„œë¡œ ìƒê°í•˜ë©´ ë¨:

1. **ì†ŒìŠ¤(source)** ì„ íƒ
   â†’ ì˜ˆ: Kinesis Data Stream (DemoStream)

2. **ëª©ì ì§€(destination)** ì„ íƒ
   â†’ ì˜ˆ: S3

3. **(ì„ íƒ) Lambdaë¡œ ë°ì´í„° ë³€í™˜ ì—¬ë¶€ ì„¤ì •**

4. **ë²„í¼ ì„¤ì •**

   * ë²„í¼ í¬ê¸°(Buffer Size):
     ë°ì´í„°ê°€ ì–¼ë§ˆë‚˜ ëª¨ì´ë©´ S3ì— ì ì§€ â†’ ê¸°ë³¸ 5MB
   * ë²„í¼ ì‹œê°„(Buffer Interval):
     ì¼ì • ì‹œê°„ë§ˆë‹¤ ê°•ì œë¡œ ë³´ë‚´ê¸° â†’ ê¸°ë³¸ 300ì´ˆ
   * ì˜ˆì œì—ì„œëŠ” ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´
     **1MB or 60ì´ˆ**ë¡œ ë³€ê²½

5. **ì••ì¶• ì—¬ë¶€** ì„¤ì • (GZIP, Snappy ë“±)

6. **ì•”í˜¸í™” ì—¬ë¶€** ì„¤ì •

7. **IAM Role ìë™ ìƒì„±**

ğŸ‘‰ ì´ ì„¤ì •ì´ ëë‚˜ë©´ FirehoseëŠ” **ìë™ìœ¼ë¡œ ë°ì´í„° íë¦„ì„ ì²˜ë¦¬í•˜ëŠ” íŒŒì´í”„ë¼ì¸**ì´ ë¨.

---

# 5) ì‹¤ì œë¡œ ë°ì´í„° íë¥´ë©´ ì–´ë–»ê²Œ ë˜ë‚˜?

í…ŒìŠ¤íŠ¸ì—ì„œëŠ”:

1. CloudShellì—ì„œ DemoStreamì—
   `signup`, `login`, `logout` ê°™ì€ ë°ì´í„° 3ê°œë¥¼ ë„£ìŒ.
2. Firehoseê°€ ì´ë¥¼ ìˆ˜ì§‘í•´ **ë²„í¼ì— ì ì‹œ ì €ì¥**
3. ë²„í¼ ì‹œê°„(60ì´ˆ)ì´ ì§€ë‚˜ë©´
   â†’ S3ì— ìë™ ì €ì¥ë¨
4. S3ì—ì„œ íŒŒì¼ í™•ì¸í•´ë³´ë©´
   ë‚ ì§œë³„ í´ë” êµ¬ì¡° ì•ˆì— ê¸°ë¡ëœ ë°ì´í„°ê°€ ë³´ì„

ğŸ‘‰ â€œë°ì´í„° â†’ Firehose â†’ S3â€ íë¦„ì´ ì˜ ì‘ë™í•œë‹¤ëŠ” ì˜ë¯¸.

---

# 6) ë§ˆë¬´ë¦¬ë¡œ ë°˜ë“œì‹œ í•´ì•¼ í•˜ëŠ” ê²ƒ

ê¸°ë³¸ì ìœ¼ë¡œ Firehoseì™€ Streamì€ **ì‹¤ì‹œê°„ìœ¼ë¡œ ê³„ì† ì‘ë™í•˜ë©´ì„œ ë¹„ìš©ì´ ë°œìƒí•˜ëŠ” ì„œë¹„ìŠ¤**ë¼ì„œ
í…ŒìŠ¤íŠ¸ ëë‚˜ë©´ **delivery stream + DemoStream ì‚­ì œ**í•´ì•¼ ìš”ê¸ˆ ì•ˆ ë‚˜ì˜´.

---

# ğŸ”‘ í•µì‹¬ ìš”ì•½ (ì´ˆê°„ë‹¨)

* **FirehoseëŠ” ì‹¤ì‹œê°„ ë°ì´í„° ìë™ ìˆ˜ì§‘ + ìë™ ì €ì¥ ì„œë¹„ìŠ¤**
* **ì›ë³¸ ë°ì´í„° â†’ (ì„ íƒ) Lambda ë³€í™˜ â†’ S3 ë“±ìœ¼ë¡œ ì €ì¥**
* ë²„í¼ í¬ê¸°/ì‹œê°„ì„ í†µí•´ **ì „ì†¡ ì†ë„ vs ë¹„ìš© ìµœì í™”** ê°€ëŠ¥
* ì••ì¶•/ì•”í˜¸í™”ë„ ìë™ ì§€ì›
* ì‹¤ìŠµì—ì„œëŠ” **Kinesis Stream â†’ Firehose â†’ S3** êµ¬ì¡°ë¡œ í…ŒìŠ¤íŠ¸í•¨

---

# Amazon Data Firehose - Hands On  
# ì•„ë§ˆì¡´ ë°ì´í„° íŒŒì´ì–´í˜¸ìŠ¤ - ì‹¤ìŠµ  

---

## Introduction to Kinesis Data Firehose Delivery Streams  
## Kinesis Data Firehose ë°°ë‹¬ ìŠ¤íŠ¸ë¦¼ ì†Œê°œ  

In this session, we will practice using Kinesis Data Firehose with delivery streams.  
ì´ë²ˆ ì„¸ì…˜ì—ì„œëŠ” Kinesis Data Firehoseë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ë‹¬ ìŠ¤íŠ¸ë¦¼ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤.  

Upon accessing the delivery streams section, we can create a new delivery stream.  
ë°°ë‹¬ ìŠ¤íŠ¸ë¦¼ ì„¹ì…˜ì— ì ‘ì†í•˜ë©´ ìƒˆ ë°°ë‹¬ ìŠ¤íŠ¸ë¦¼ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

A detailed diagram illustrates how Kinesis Data Firehose operates.  
ìì„¸í•œ ë‹¤ì´ì–´ê·¸ë¨ì€ Kinesis Data Firehoseì˜ ì‘ë™ ë°©ì‹ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  

---

## Data Sources  
## ë°ì´í„° ì†ŒìŠ¤  

Data is ingested from producers, which can be either a Kinesis Data Stream, as in our use case, or Direct PUTs through Kinesis Data Agents, other AWS services such as CloudWatch, IoT Core, EventBridge, and also custom applications using the SDK that send data directly into Kinesis Data Firehose.  
ë°ì´í„°ëŠ” í”„ë¡œë“€ì„œì—ì„œ ìˆ˜ì§‘ë˜ë©°, í”„ë¡œë“€ì„œëŠ” Kinesis Data Stream(ì´ ì‹¤ìŠµì—ì„œëŠ” ì‚¬ìš©), Kinesis Data Agentë¥¼ í†µí•œ Direct PUT, CloudWatch, IoT Core, EventBridge ë“±ì˜ AWS ì„œë¹„ìŠ¤, ë˜ëŠ” SDKë¥¼ ì‚¬ìš©í•´ ì§ì ‘ Firehoseë¡œ ë°ì´í„°ë¥¼ ì „ì†¡í•˜ëŠ” ì»¤ìŠ¤í…€ ì• í”Œë¦¬ì¼€ì´ì…˜ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

Once data is ingested, it can be transformed using a Lambda function.  
ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ë©´ Lambda í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

This transformation can include converting the record format or other processing before loading the data into target stores.  
ì´ ë³€í™˜ ê³¼ì •ì—ëŠ” ë ˆì½”ë“œ í¬ë§· ë³€í™˜ì´ë‚˜ ëŒ€ìƒ ì €ì¥ì†Œì— ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê¸° ì „ì˜ ê¸°íƒ€ ì²˜ë¦¬ ì‘ì—…ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

The data is then loaded into destinations such as Amazon S3, Amazon OpenSearch Service (formerly ElasticSearch), Amazon Redshift, or various HTTP endpoint destinations.  
ê·¸ í›„ ë°ì´í„°ëŠ” Amazon S3, Amazon OpenSearch Service(êµ¬ ElasticSearch), Amazon Redshift, ë˜ëŠ” ë‹¤ì–‘í•œ HTTP ì—”ë“œí¬ì¸íŠ¸ ëŒ€ìƒì§€ë¡œ ë¡œë“œë©ë‹ˆë‹¤.  

In this example, our source will be a Kinesis Data Stream, and the destination will be Amazon S3.  
ì´ ì˜ˆì œì—ì„œëŠ” ì†ŒìŠ¤ë¡œ Kinesis Data Streamì„ ì‚¬ìš©í•˜ê³ , ëŒ€ìƒì§€ëŠ” Amazon S3ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.  

It is important to remember the key destinations: OpenSearch Service, Redshift, and S3.  
ì£¼ìš” ëŒ€ìƒì§€ì¸ OpenSearch Service, Redshift, S3ë¥¼ ê¸°ì–µí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.  

There are also many third-party services and custom HTTP endpoints available, but these are less critical to memorize.  
ë˜í•œ ë§ì€ íƒ€ì‚¬ ì„œë¹„ìŠ¤ì™€ ì»¤ìŠ¤í…€ HTTP ì—”ë“œí¬ì¸íŠ¸ê°€ ìˆì§€ë§Œ, ì™¸ìš°ëŠ” ê²ƒì´ í•„ìˆ˜ì ì´ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.  

---

## Creating a Delivery Stream  
## ë°°ë‹¬ ìŠ¤íŠ¸ë¦¼ ìƒì„±  

We select Amazon S3 as the destination.  
ëŒ€ìƒì§€ë¡œ Amazon S3ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.  

For the source, we browse and choose our stream, which is the ARN DemoStream in this case.  
ì†ŒìŠ¤ë¡œëŠ” ARN DemoStream ìŠ¤íŠ¸ë¦¼ì„ ì„ íƒí•©ë‹ˆë‹¤.  

The delivery stream name is automatically generated.  
ë°°ë‹¬ ìŠ¤íŠ¸ë¦¼ ì´ë¦„ì€ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤.  

---

## Transform and Convert Records  
## ë ˆì½”ë“œ ë³€í™˜ ë° í¬ë§· ë³€ê²½  

Transforming source records using Lambda is optional but useful.  
Lambdaë¥¼ ì‚¬ìš©í•œ ì†ŒìŠ¤ ë ˆì½”ë“œ ë³€í™˜ì€ ì„ íƒ ì‚¬í•­ì´ì§€ë§Œ ìœ ìš©í•©ë‹ˆë‹¤.  

Lambda functions are pieces of code that run in AWS and can perform operations such as filtering, decompressing, converting, or processing source records before delivery by Kinesis Data Firehose.  
Lambda í•¨ìˆ˜ëŠ” AWSì—ì„œ ì‹¤í–‰ë˜ëŠ” ì½”ë“œ ì¡°ê°ìœ¼ë¡œ, Kinesis Data Firehoseê°€ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ê¸° ì „ í•„í„°ë§, ì••ì¶• í•´ì œ, í¬ë§· ë³€í™˜ ë˜ëŠ” ì²˜ë¦¬ ë“±ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

---

### Record Format Conversion  
### ë ˆì½”ë“œ í¬ë§· ë³€í™˜  

Depending on the destination, it can be beneficial to convert record formats into Parquet or ORC using advanced options.  
ëŒ€ìƒì§€ì— ë”°ë¼, ê³ ê¸‰ ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬ ë ˆì½”ë“œ í¬ë§·ì„ Parquet ë˜ëŠ” ORCë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì´ ìœ ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

This topic is more detailed in the AWS data and analytics certification and is beyond the current scope.  
ì´ ì£¼ì œëŠ” AWS ë°ì´í„° ë° ë¶„ì„ ì¸ì¦ ê³¼ì •ì—ì„œ ìì„¸íˆ ë‹¤ë£¨ë©°, í˜„ì¬ ì‹¤ìŠµ ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤.  

---

## Destination Configuration  
## ëŒ€ìƒì§€ êµ¬ì„±  

We choose an existing S3 bucket named demo-firehose-stephane-V3.  
ê¸°ì¡´ S3 ë²„í‚· demo-firehose-stephane-V3ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.  

Dynamic partitioning is set to no, and no prefix is added to the bucket.  
ë™ì  íŒŒí‹°ì…”ë‹ì€ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©°, ë²„í‚·ì— ì ‘ë‘ì‚¬ë„ ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  

An error output prefix is also not configured to keep the setup simple.  
ì„¤ì •ì„ ë‹¨ìˆœí•˜ê²Œ ìœ ì§€í•˜ê¸° ìœ„í•´ ì˜¤ë¥˜ ì¶œë ¥ ì ‘ë‘ì‚¬ë„ ì„¤ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  

---

## Buffer Hints, Compression, and Encryption  
## ë²„í¼ íŒíŠ¸, ì••ì¶• ë° ì•”í˜¸í™”  

The buffer allows Kinesis Data Firehose to accumulate records before delivering them to the target.  
ë²„í¼ëŠ” Kinesis Data Firehoseê°€ ë°ì´í„°ë¥¼ ëŒ€ìƒì§€ë¡œ ì „ë‹¬í•˜ê¸° ì „ì— ë ˆì½”ë“œë¥¼ ëˆ„ì í•˜ë„ë¡ í•©ë‹ˆë‹¤.  

By default, the buffer size is five megabytes.  
ê¸°ë³¸ ë²„í¼ í¬ê¸°ëŠ” 5MBì…ë‹ˆë‹¤.  

You can increase the buffer size for efficiency or decrease it for faster delivery.  
íš¨ìœ¨ì„±ì„ ìœ„í•´ ë²„í¼ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜, ë¹ ë¥¸ ì „ë‹¬ì„ ìœ„í•´ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

Here, the buffer size is set to 1 MB for speed.  
ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” ë¹ ë¥¸ ì „ë‹¬ì„ ìœ„í•´ ë²„í¼ í¬ê¸°ë¥¼ 1MBë¡œ ì„¤ì •í•©ë‹ˆë‹¤.  

The buffer interval controls how quickly data is flushed if the buffer size is not reached.  
ë²„í¼ ê°„ê²©ì€ ë²„í¼ í¬ê¸°ê°€ ë„ë‹¬í•˜ì§€ ì•Šì•˜ì„ ë•Œ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ í”ŒëŸ¬ì‹œí• ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.  

For example, a 300-second interval means the buffer flushes every five minutes regardless of size.  
ì˜ˆë¥¼ ë“¤ì–´, 300ì´ˆ ê°„ê²©ì´ë©´ ë²„í¼ëŠ” í¬ê¸°ì™€ ê´€ê³„ì—†ì´ 5ë¶„ë§ˆë‹¤ í”ŒëŸ¬ì‹œë©ë‹ˆë‹¤.  

We set the buffer interval to 60 seconds to ensure data is delivered at least every minute.  
ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” ìµœì†Œ 1ë¶„ë§ˆë‹¤ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ë„ë¡ 60ì´ˆë¡œ ì„¤ì •í•©ë‹ˆë‹¤.  

Compression options include GZIP, Snappy, Zip, or Hadoop-Compatible Snappy, which help save storage space by compressing data before storing it in Amazon S3.  
ì••ì¶• ì˜µì…˜ì—ëŠ” GZIP, Snappy, Zip, Hadoop í˜¸í™˜ Snappyê°€ ìˆìœ¼ë©°, Amazon S3ì— ì €ì¥í•˜ê¸° ì „ì— ë°ì´í„°ë¥¼ ì••ì¶•í•´ ì €ì¥ ê³µê°„ì„ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

Encryption can also be enabled for securing records.  
ë ˆì½”ë“œ ë³´ì•ˆì„ ìœ„í•´ ì•”í˜¸í™”ë„ í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

An IAM role is automatically created with the necessary permissions to write to Amazon S3 and read from the Kinesis Data Stream.  
S3 ì“°ê¸° ë° Kinesis Data Stream ì½ê¸° ê¶Œí•œì„ ê°€ì§„ IAM ì—­í• ì´ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤.  

This role enables Kinesis Data Firehose to interact with the target buckets securely.  
ì´ ì—­í• ì€ Kinesis Data Firehoseê°€ ëŒ€ìƒ ë²„í‚·ê³¼ ì•ˆì „í•˜ê²Œ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.  

After configuring these settings, we create the delivery stream.  
ì„¤ì • í›„ ë°°ë‹¬ ìŠ¤íŠ¸ë¦¼ì„ ìƒì„±í•©ë‹ˆë‹¤.  

Once active, metrics become available to monitor data flow, which is useful in production environments.  
í™œì„±í™”ë˜ë©´ ë°ì´í„° íë¦„ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ì§€í‘œê°€ ì œê³µë˜ë©°, ì´ëŠ” ìš´ì˜ í™˜ê²½ì—ì„œ ìœ ìš©í•©ë‹ˆë‹¤.  

---

## Testing Data Flow  
## ë°ì´í„° íë¦„ í…ŒìŠ¤íŠ¸  

Our Kinesis Data Firehose source is the Kinesis Data Stream named DemoStream.  
Kinesis Data Firehose ì†ŒìŠ¤ëŠ” DemoStreamì´ë¼ëŠ” Kinesis Data Streamì…ë‹ˆë‹¤.  

To activate Firehose, new data must be sent to the stream after setup.  
Firehoseë¥¼ í™œì„±í™”í•˜ë ¤ë©´ ì„¤ì • í›„ ìŠ¤íŠ¸ë¦¼ì— ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì „ì†¡í•´ì•¼ í•©ë‹ˆë‹¤.  

Using CloudShell, we send test data to the DemoStream.  
CloudShellì„ ì‚¬ìš©í•´ DemoStreamìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.  

The data includes events such as user signup, user login, and user logout.  
ë°ì´í„°ì—ëŠ” ì‚¬ìš©ì ê°€ì…, ë¡œê·¸ì¸, ë¡œê·¸ì•„ì›ƒ ì´ë²¤íŠ¸ê°€ í¬í•¨ë©ë‹ˆë‹¤.  

After sending three records, we check the Amazon S3 bucket.  
ì„¸ ê°œì˜ ë ˆì½”ë“œë¥¼ ì „ì†¡í•œ í›„ Amazon S3 ë²„í‚·ì„ í™•ì¸í•©ë‹ˆë‹¤.  

Initially, there are zero objects because the buffer interval is 60 seconds.  
ì²˜ìŒì—ëŠ” ë²„í¼ ê°„ê²©ì´ 60ì´ˆì´ë¯€ë¡œ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤.  

We wait for the buffer to flush the data into S3.  
ë²„í¼ê°€ ë°ì´í„°ë¥¼ S3ë¡œ í”ŒëŸ¬ì‹œí•  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.  

After more than 60 seconds, refreshing the S3 bucket shows the new data files partitioned by date.  
60ì´ˆ ì´ìƒ ì§€ë‚œ í›„ S3 ë²„í‚·ì„ ìƒˆë¡œê³ ì¹¨í•˜ë©´ ë‚ ì§œë³„ë¡œ íŒŒí‹°ì…”ë‹ëœ ìƒˆ ë°ì´í„° íŒŒì¼ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.  

Opening a file reveals the records with user signup, login, and logout events.  
íŒŒì¼ì„ ì—´ë©´ ì‚¬ìš©ì ê°€ì…, ë¡œê·¸ì¸, ë¡œê·¸ì•„ì›ƒ ì´ë²¤íŠ¸ ë ˆì½”ë“œê°€ í™•ì¸ë©ë‹ˆë‹¤.  

This confirms that Kinesis Data Firehose is working correctly and delivering data to Amazon S3 as expected.  
ì´ëŠ” Kinesis Data Firehoseê°€ ì •ìƒ ì‘ë™í•˜ë©° ë°ì´í„°ë¥¼ ì˜ˆìƒëŒ€ë¡œ Amazon S3ë¡œ ì „ë‹¬í•¨ì„ í™•ì¸ì‹œì¼œì¤ë‹ˆë‹¤.  

---

## Cleanup  
## ì •ë¦¬  

To avoid incurring charges, it is important to delete the delivery stream and the DemoStream after finishing the demo.  
ë¹„ìš© ë°œìƒì„ ë°©ì§€í•˜ë ¤ë©´ ì‹¤ìŠµ í›„ ë°°ë‹¬ ìŠ¤íŠ¸ë¦¼ê³¼ DemoStreamì„ ì‚­ì œí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.  

This prevents ongoing costs associated with running these services.  
ì´ë ‡ê²Œ í•˜ë©´ í•´ë‹¹ ì„œë¹„ìŠ¤ë¥¼ ê³„ì† ì‚¬ìš©í•˜ëŠ” ë° ë”°ë¥¸ ë¹„ìš©ì´ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  

This concludes the lecture on using Amazon Kinesis Data Firehose with delivery streams.  
ì´ë¡œì¨ ë°°ë‹¬ ìŠ¤íŠ¸ë¦¼ì„ ì‚¬ìš©í•œ Amazon Kinesis Data Firehose ì‹¤ìŠµ ê°•ì˜ë¥¼ ë§ˆì¹©ë‹ˆë‹¤.  

Thank you for following along.  
ì°¸ì—¬í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.  

---

## Key Takeaways  
## í•µì‹¬ ìš”ì•½  

- Kinesis Data Firehose ingests data from various sources including Kinesis Data Streams, AWS services, and custom applications.  
- Kinesis Data FirehoseëŠ” Kinesis Data Streams, AWS ì„œë¹„ìŠ¤, ì»¤ìŠ¤í…€ ì• í”Œë¦¬ì¼€ì´ì…˜ ë“± ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.  

- Data can be transformed using Lambda functions before delivery to destinations such as Amazon S3, OpenSearch Service, and Redshift.  
- ë°ì´í„°ëŠ” Amazon S3, OpenSearch Service, Redshift ë“±ì˜ ëŒ€ìƒì§€ë¡œ ì „ë‹¬ë˜ê¸° ì „ì— Lambda í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

- Buffer size and interval settings control how frequently data is delivered to the target, balancing efficiency and latency.  
- ë²„í¼ í¬ê¸°ì™€ ê°„ê²© ì„¤ì •ì€ ë°ì´í„° ì „ë‹¬ ë¹ˆë„ë¥¼ ì¡°ì ˆí•˜ì—¬ íš¨ìœ¨ì„±ê³¼ ì§€ì—° ì‹œê°„ì˜ ê· í˜•ì„ ë§ì¶¥ë‹ˆë‹¤.  

- Compression and encryption options help optimize storage costs and secure data in transit and at rest.  
- ì••ì¶• ë° ì•”í˜¸í™” ì˜µì…˜ì€ ì €ì¥ ë¹„ìš©ì„ ìµœì í™”í•˜ê³  ë°ì´í„° ì „ì†¡ ë° ì €ì¥ ì‹œ ë³´ì•ˆì„ ê°•í™”í•©ë‹ˆë‹¤.  
